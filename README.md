
## Contacts

[Telegram channel](https://t.me/+0G5ev2gTmEo3ODYy)

**Lecturers**: Polina Polunina, Semeon Budennyy

**Class Teachers and TAs**

| Class Teachers | Group| TA (contact)|  
|----------------|------|-------|
|Andrei Egorov|БПИ201, БПИ202|Andrei Dyadynov (tg: @mr_dyadyunov), Nikita Tatarinov (tg: @NickyOL)|
|Kirill Bykov|БПИ203, БПИ204| Anastasia Egorova (tg: @wwhatisitt), Elizaveta Berdina (tg: @berdina_elis)|
|Maria Tikhonova|БПИ205|Alexander Stepin (tg: @kevicia)|
|Anastasia Voronkova|БПИ206, БПИ207| Anton Alekseev (tg: @flameglamebeatskilla), Emil Akopyan (tg: @archivarius)|


## Recomended Literature

[PR] Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.\
[Link](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)

[ESL] Hastie, T., Hastie, T., Tibshirani, R., & Friedman, J. H. (2001). The elements of statistical learning: Data mining, inference, and prediction. New York: Springer.\
[Link](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)

[FML] Mohri, M., Talwalkar, A., & Rostamizadeh, A. Second Edition, (2018). Foundations of Machine Learning. Cambridge, MA: The MIT Press.\
[Link](https://cs.nyu.edu/~mohri/mlbook/)

## Class materials

#### Lectures


| Date | Topic | Lecture materials| Reading|
|------|-------|------------------|--------|
|5 sep|1.Introduction|  |[FML] Ch 1; [ESL] Ch 2.1-2 |
|12 sep|2.Gradient Optimization|  | [FML] Appx A, B; [Convex Optimization book](https://web.stanford.edu/~boyd/cvxbook/)|
|19 sep|3.Linear Regression|  |[PR] Ch 3.1; [ESL] Ch 3.1-4;  [FML] Ch 4.4-6|
|26 sep|4.Linear Classification|  |[PR] Ch 4.1;  [ESL] Ch 4.1-2, 4.4; [FML] Ch 13|   
|3 oct|5.Logistic Regression and SVM|  |[ESL] Ch 12.1-3; [FML] Ch 5, 6  |
|10 oct|6.Decision Trees|  | [ESL] Ch 9.2|
|17 oct|7.Bagging, Random Forest| |[PR] Ch 3.2 (bias-variance); [ESL] Ch 8;  [FML] Ch 7|
|24 oct - 30 oct| NO LECTURES | --- | --- |
|31 oct|8.Gradient boosting|  |  [PR] Ch 14.3; [ESL] Ch 10|
|7 nov|9.Clustering and Anomaly Detection |  |[PR] Ch 9.1; [ESL] Ch 13.2, 14.3  |
|14 nov|10.Dimensionality reduction: PCA, SVD |  | [ESL] Ch 14.5; [PR] Ch 12.1 |
|21 nov|11.Testing your models: AA/AB tests | | |
|28 nov|12.MLP and basic NN |  | [PR] Ch 5.1-5.5; [ESL] Ch 11  |
|5 dec|13.Basic CV: convolutional layer |  |  |
|12 dec|14.ML: business applications |  |  |
|19 dec|15.Summary |  |  |



#### Practicals

| Date | Topic | Materials| Extra Reading/Practice|  
|------|-------|----------|-----------------------|
|6-10 sep|1.Basic toolbox| [Notebook](week01/01_HSE_PE_Intro_to_Python_v4.ipynb); [Dataset](week01/dpo_1-2_winemag-data_first150k.csv)|[Python Crash Course](week01/Additional_notebooks/)|
|13-17 sep|2.EDA and Scikit-learn| [Notebook](week02/02_HSE_SE_EDA_v1.ipynb) ||
|20-24 sep|3.Calculus background: Matrix-Vec differention and GD|[Notebook](week03/03_HSE_SE_GD.ipynb); [Matrix-vector differentiation](week03/sem03-vector-diff.pdf) |[The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi//matrixcookbook.pdf)|
|27-1 oct|4.Linear Regression| [Notebook](week04/04_HSE_SE_Linear_regression_v3.ipynb) ||
|4-8 oct|5.Classification metrics| [Notebook](week05/05_HSE_PE_Classification_v2.ipynb)  ||
|11-15 oct|6.NLP & SVM|[Notebook](week06/06_HSE_SE_intro_to_NLP_SVM.ipynb)  |[NLP For You - great online course](https://lena-voita.github.io/nlp_course.html#main_page_content)|
|18-22 oct|7.Decision Trees|[Notebook](week07/07_HSE_SE_DT.ipynb)  |[Guide2DT](https://odsc.medium.com/the-complete-guide-to-decision-trees-part-1-aa68b34f476d)|
|1-5 nov|8.Ensembles|  | |
|8-12 nov|9.Gradient Boosting |   |  |
|15-19 nov|10.Anomaly detection and Clustering |  |  |
|22-26 nov|11.Dimension reduction: PCA, SVD |  |  |
|29-3 dec|12.AA/AB tests |  |  |
|6-10 dec|13.MLP and basic NN  |   |  |
|13-17 dec|14.Basic CV: convolutional layer |   |  |
|20-24 dec|15.Exam preparation, summary |   |  |



## Grading
```Final grade = 0.7*HW + 0.3*Exam```

* `HW` - Average grade for the assignments 1 to 5. 
You can get extra points by solving HW 6, but no more than 10 in total. Namely, `HW = (hw1 + hw2 + hw3 + hw4 + hw5 + hw6)/5`


* `Exam` -  Grade for the exam. 
 ---
 
You can skip the exam if all grades for the assignemnts are **not smaller** than 5.5, i.e. (`hw >=5.5`). 
In this case:

```Final grade = ROUND(HW)```
