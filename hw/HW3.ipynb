{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3v5HIUdDvY5"
   },
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 3\n",
    "\n",
    "**Warning 1**: some problems require (especially the lemmatization part) significant amount of time, so **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7t9dYtdDvZC"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIHtwV6vDvZD"
   },
   "source": [
    "## PART 1: Logit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7XKEWcVDvZD"
   },
   "source": [
    "We consider a binary classification problem. For prediction, we would like to use a logistic regression model. For regularization we add a combination of the $l_2$ and $l_1$ penalties (Elastic Net). \n",
    "\n",
    "Each object in the training dataset is indexed with $i$ and described by pair: features $x_i\\in\\mathbb{R}^{K}$ and binary labels $y_i$. The model parametrized with bias $w_0\\in\\mathbb{R}$ and weights $w\\in\\mathbb{R}^K$.\n",
    "\n",
    "The optimization problem with respect to the $w_0, w$ is the following (Elastic Net Loss):\n",
    "\n",
    "$$L(w, w_0) = \\frac{1}{N} \\sum_{i=1}^N \\ln(1+\\exp(-y_i(w^\\top x_i+w_0))) + \\gamma \\|w\\|_1 + \\beta \\|w\\|_2^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1eSuDKXFVZu"
   },
   "source": [
    "#### 1. [0.5 points]  Find the gradient of the Elastic Net loss and write its formulas (better in latex format) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zjH-YnPDvZD"
   },
   "source": [
    "##### Put your markdown formulas here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_lIccN_DvZE"
   },
   "source": [
    "#### 2. [0.25 points] Implement the Elastic Net loss (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9QNfCtV5DvZE"
   },
   "outputs": [],
   "source": [
    "def loss(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> float:\n",
    "    return #...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIVoC6UmDvZE"
   },
   "source": [
    "#### 3. [0.25 points] Implement the gradient (as a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWqBLGRADvZE"
   },
   "outputs": [],
   "source": [
    "def get_grad(X, y, w: List[float], w0: float, gamma=1., beta=1.) -> Tuple[List[float], float]:\n",
    "    grad_w0 = #...\n",
    "    grad_w = #...\n",
    "    \n",
    "    return grad_w, grad_w0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhOb8HrtDvZF"
   },
   "source": [
    "#### Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FxXTocHDvZF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.multivariate_normal(np.arange(5), np.eye(5), size=10)\n",
    "y = np.random.binomial(1, 0.42, size=10)\n",
    "w, w0 = np.random.normal(size=5), np.random.normal()\n",
    "\n",
    "grad_w, grad_w0 = get_grad(X, y, w, w0)\n",
    "assert(np.allclose(grad_w,\n",
    "                   [-2.73262076, -1.87176281, 1.30051144, 2.53598941, -2.71198109],\n",
    "                   rtol=1e-2) & \\\n",
    "       np.allclose(grad_w0,\n",
    "                   -0.2078231418067844, \n",
    "                   rtol=1e-2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbqLfcrRDvZF"
   },
   "source": [
    "####  4. [1 point]  Implement gradient descent which works for both tol level and max_iter stop criteria and plot the decision boundary of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIgiwQkjDvZF"
   },
   "source": [
    "The template provides basic sklearn API class. You are free to modify it in any convenient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Thyeux0KDvZG"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erVmNR0PDvZG"
   },
   "outputs": [],
   "source": [
    "class Logit(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, beta=1.0, gamma=1.0, lr=1e-3, tolerance=0.01, max_iter=1000, random_state=42):  \n",
    "        self.beta = beta        \n",
    "        self.gamma = gamma\n",
    "        self.tolerance= tolerance\n",
    "        self.max_iter= max_iter\n",
    "        self.learning_rate = lr\n",
    "        self.random_state = random_state\n",
    "        # you may additional properties if you wish\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # add weights and bias and optimize Elastic Net loss over (X,y) dataset\n",
    "        # save history of optimization steps\n",
    "        \n",
    "        # your code here\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # return vector of predicted labels for each object from X\n",
    "        # your code here\n",
    "\n",
    "        return predict\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        return np.array([1 / (1 + np.exp(np.dot(X, self.w) + self.w0)),\\\n",
    "                         1 / (1 + np.exp(-np.dot(X, self.w) - self.w0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7SJX8Y6EDvZG"
   },
   "outputs": [],
   "source": [
    "# sample data to test your model\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=180, n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=42, n_clusters_per_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u41kzwGTDvZH"
   },
   "outputs": [],
   "source": [
    "# a function to plot the decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    fig = plt.figure()\n",
    "    X1min, X2min = X.min(axis=0)\n",
    "    X1max, X2max = X.max(axis=0)\n",
    "    x1, x2 = np.meshgrid(np.linspace(X1min, X1max, 200),\n",
    "                         np.linspace(X2min, X2max, 200))\n",
    "    ypred = model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    ypred = ypred.reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, ypred, alpha=.4)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNuYbsAoDvZI"
   },
   "outputs": [],
   "source": [
    "model = Logit(0,0)\n",
    "y[y == 0] = -1\n",
    "model.fit(X, y)\n",
    "plot_decision_boundary(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qi4WRhcADvZI"
   },
   "source": [
    "#### 5. [0.25 points] Plot loss diagram for the model, i.e. show the dependence of the loss function from the gradient descent steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyjMDAKuDvZI"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FhSCAv_DvZJ"
   },
   "source": [
    "## PART 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYyGsSxEDvZJ"
   },
   "source": [
    "#### 6. [2 point] Using the same dataset, train SVM Classifier from Sklearn.\n",
    "Investigate how different parameters influence the quality of the solution:\n",
    "+ Try several kernels: Linear, Polynomial, RBF (and others if you wish). Some Kernels have hypermeters: don't forget to try different.\n",
    "+ Regularization coefficient \n",
    "\n",
    "Show how these parameters affect accuracy, roc_auc and f1 score. \n",
    "Make plots for the dependencies between metrics and parameters. \n",
    "Try to formulate conclusions from the observations. How sensitive are kernels to hyperparameters? How sensitive is a solution to the regularization? Which kernel is prone to overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nicu_O3IDvZK"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sY8q6JdCDvZK"
   },
   "source": [
    "## PART 3: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD4xKhYfDvZK"
   },
   "source": [
    "#### 7. [1.75 point] Form the dataset\n",
    "\n",
    "We are going to form a dataset that we will use in the following tasks for binary and multiclass classification\n",
    "\n",
    "0. Choose **six** authors that you like (specify who you've chosen) and download the <a href=\"https://www.kaggle.com/d0rj3228/russian-literature?select=prose\">relevant data</a> from **prose** section\n",
    "1. Build your own dataset for these authors: \n",
    "    * divide each text into sentences such that we will have two columns: *sentence* and *target author*, each row will contain one sentence and one target\n",
    "    * drop sentences where N symbols in a sentence < 15\n",
    "    * fix random state and randomly choose sentences in the folowing proportion \"5k : 15k : 8k : 11k : 20k : 3k\" for the authors respectively\n",
    "    \n",
    "    sample data may look like:\n",
    "    \n",
    "    <center> \n",
    "    <table>\n",
    "        <tr>\n",
    "            <th> sentence </th>\n",
    "            <th> author </th>\n",
    "        </tr> \n",
    "        <tr><td> Несколько лет тому назад в одном из своих поместий жил старинный русской барин, Кирила Петрович Троекуров. </td><td> Пушкин </td><td> \n",
    "        <tr><td> Уже более недели приезжий господин жил в городе, разъезжая по вечеринкам и обедам и таким образом проводя, как говорится, очень приятно время. </td><td> Гоголь </td><td> \n",
    "        <tr><td> ... </td><td> ... </td><td> \n",
    "        <tr><td> Я жил недорослем, гоняя голубей и играя в чехарду с дворовыми мальчишками. </td><td> Пушкин </td><td>         \n",
    "    </table>\n",
    "</center>\n",
    "     \n",
    "2. Preprocess (tokenize and clean) the dataset \n",
    "    * tokenize, remove all stop words (nltk.corpus.stopwords), punctuation (string.punctuation) and numbers\n",
    "    * convert to lower case and apply either stemming or lemmatization of the words (on your choice)\n",
    "    * vectorize words using both **bag of words** and **tf-idf** (use sklearn)\n",
    "    * observe and describe the difference between vectorized output (what do numbers look like after transformations and what do they represent?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVStbeQ8DvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuTi3rvnDvZL"
   },
   "source": [
    "###  Binary classification\n",
    "\n",
    "#### 8. [2 point] Train model using Logistic Regression (your own) and SVC (SVM can be taken from sklearn) \n",
    "\n",
    "* choose *two* authors from the dataset that you have formed in the previous task\n",
    "* check the balance of the classes\n",
    "* divide the data into train and test samples with 0.7 split rate (don't forget to fix the random state)\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score) and use it in the next tasks\n",
    "* make several plots to address the dependence between F1 score and parameters\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute some relevant metrics for test sample (useful to check the seminars 5 and 6, use sklearn) \n",
    "* make conclusions about the performance of your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZUP1HqFDvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G1kt0qbDvZL"
   },
   "source": [
    "#### 9. [1 point] Analysing ROC AUC\n",
    "\n",
    "It is possible to control the proportion of statistical errors of different types using different thresholds for choosing a class. Plot ROC curves for Logistic Regression and SVC, show the threshold on ROC curve plots. Choose such a threshold that your models have no more than 30% of false positive errors rate. Pay attention to `thresholds` parameter in sklearn roc_curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZ2GZ8-uDvZL"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-qubaK4DvZM"
   },
   "source": [
    "### Multiclass logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJQone-LDvZM"
   },
   "source": [
    "#### 10. [1 point] Take the One-VS-One classifier (use sklearn) and apply to Logit model (one you've made in the 4th task) in order to get multiclass linear classifier\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html\">OneVsOneClassifier</a>\n",
    "\n",
    "* use the data you got at the previous step for 6 authors\n",
    "* divide the data into train and test samples with 0.7 split rate\n",
    "* using GridSearchCV - find the best parameters for the models (by F1 score)\n",
    "* plot confusion matrix for train and test samples\n",
    "* compute all possible and relevant metrics for test sample (use sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4lR8qJ7DvZM"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_v7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
